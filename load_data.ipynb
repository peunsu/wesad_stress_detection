{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282abc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6889394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECT_IDS = [f'S{i}' for i in range(2, 12)] + [f'S{i}' for i in range(13, 18)]  # S2 to S11 and S13 to S17\n",
    "TEST_SUBJECTS = 5\n",
    "SAMPLING_RATE = 4\n",
    "SAMPLING_RATES = {\n",
    "    'chest': {\n",
    "        'ACC': 700,\n",
    "        'ECG': 700,\n",
    "        'EMG': 700,\n",
    "        'EDA': 700,\n",
    "        'Resp': 700,\n",
    "        'Temp': 700\n",
    "    },\n",
    "    'wrist': {\n",
    "        'ACC': 32,\n",
    "        'BVP': 64,\n",
    "        'EDA': 4,\n",
    "        'TEMP': 4\n",
    "    },\n",
    "    'label': 700\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d2f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecg_process(ecg_signal, sampling_rate=1000, method=\"neurokit\", **kwargs):\n",
    "    # Sanitize and clean input\n",
    "    ecg_signal = nk.signal.signal_sanitize(ecg_signal)\n",
    "    ecg_cleaned = ecg_signal\n",
    "\n",
    "    # Detect R-peaks\n",
    "    instant_peaks, info = nk.ecg_peaks(\n",
    "        ecg_cleaned=ecg_cleaned,\n",
    "        sampling_rate=sampling_rate,\n",
    "        method=\"rodrigues2021\",\n",
    "        correct_artifacts=True,\n",
    "    )\n",
    "\n",
    "    # Calculate heart rate\n",
    "    rate = nk.signal.signal_rate(\n",
    "        info, sampling_rate=sampling_rate, desired_length=len(ecg_cleaned)\n",
    "    )\n",
    "\n",
    "    # Assess signal quality\n",
    "    quality = nk.ecg_quality(\n",
    "        ecg_cleaned, rpeaks=info[\"ECG_R_Peaks\"], sampling_rate=sampling_rate\n",
    "    )\n",
    "\n",
    "    # Merge signals in a DataFrame\n",
    "    signals = pd.DataFrame(\n",
    "        {\n",
    "            \"ECG_Raw\": ecg_signal,\n",
    "            \"ECG_Clean\": ecg_cleaned,\n",
    "            \"ECG_Rate\": rate,\n",
    "            \"ECG_Quality\": quality,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Delineate QRS complex\n",
    "    delineate_signal, delineate_info = nk.ecg_delineate(\n",
    "        ecg_cleaned=ecg_cleaned, rpeaks=info[\"ECG_R_Peaks\"], sampling_rate=sampling_rate\n",
    "    )\n",
    "    info.update(delineate_info)  # Merge waves indices dict with info dict\n",
    "\n",
    "    # Determine cardiac phases\n",
    "    cardiac_phase = nk.ecg_phase(\n",
    "        ecg_cleaned=ecg_cleaned,\n",
    "        rpeaks=info[\"ECG_R_Peaks\"],\n",
    "        delineate_info=delineate_info,\n",
    "    )\n",
    "\n",
    "    # Add additional information to signals DataFrame\n",
    "    signals = pd.concat(\n",
    "        [signals, instant_peaks, delineate_signal, cardiac_phase], axis=1\n",
    "    )\n",
    "\n",
    "    # return signals DataFrame and R-peak locations\n",
    "    return signals, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949aaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emg_process(emg_signal, sampling_rate=1000, report=None, **kwargs):\n",
    "    # Sanitize input\n",
    "    emg_signal = nk.signal.signal_sanitize(emg_signal)\n",
    "    methods = nk.emg.emg_methods.emg_methods(sampling_rate=sampling_rate, **kwargs)\n",
    "\n",
    "    # Clean signal\n",
    "    emg_cleaned = emg_signal\n",
    "\n",
    "    # Get amplitude\n",
    "    amplitude = nk.emg_amplitude(emg_cleaned)\n",
    "\n",
    "    # Get onsets, offsets, and periods of activity\n",
    "    activity_signal, info = nk.emg_activation(\n",
    "        emg_amplitude=amplitude,\n",
    "        emg_cleaned=emg_cleaned,\n",
    "        sampling_rate=sampling_rate,\n",
    "        method=methods[\"method_activation\"],\n",
    "        **methods[\"kwargs_activation\"]\n",
    "    )\n",
    "    info[\"sampling_rate\"] = sampling_rate  # Add sampling rate in dict info\n",
    "\n",
    "    # Prepare output\n",
    "    signals = pd.DataFrame(\n",
    "        {\"EMG_Raw\": emg_signal, \"EMG_Clean\": emg_cleaned, \"EMG_Amplitude\": amplitude}\n",
    "    )\n",
    "\n",
    "    signals = pd.concat([signals, activity_signal], axis=1)\n",
    "\n",
    "    return signals, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1f116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_process(\n",
    "    eda_signal, sampling_rate=1000, method=\"neurokit\", report=None, **kwargs\n",
    "):\n",
    "    # Sanitize input\n",
    "    eda_signal = nk.signal.signal_sanitize(eda_signal)\n",
    "    methods = nk.eda.eda_methods.eda_methods(sampling_rate=sampling_rate, method=method, **kwargs)\n",
    "\n",
    "    # Preprocess\n",
    "    # Clean signal\n",
    "    eda_cleaned = eda_signal\n",
    "    if methods[\"method_phasic\"] is None or methods[\"method_phasic\"].lower() == \"none\":\n",
    "        eda_decomposed = pd.DataFrame({\"EDA_Phasic\": eda_cleaned})\n",
    "    else:\n",
    "        eda_decomposed = nk.eda_phasic(\n",
    "            eda_cleaned,\n",
    "            sampling_rate=sampling_rate,\n",
    "            method=methods[\"method_phasic\"],\n",
    "            **methods[\"kwargs_phasic\"],\n",
    "        )\n",
    "\n",
    "    # Find peaks\n",
    "    peak_signal, info = nk.eda_peaks(\n",
    "        eda_decomposed[\"EDA_Phasic\"].values,\n",
    "        sampling_rate=sampling_rate,\n",
    "        method=methods[\"method_peaks\"],\n",
    "        amplitude_min=0.1,\n",
    "        **methods[\"kwargs_peaks\"],\n",
    "    )\n",
    "    info[\"sampling_rate\"] = sampling_rate  # Add sampling rate in dict info\n",
    "\n",
    "    # Store\n",
    "    signals = pd.DataFrame({\"EDA_Raw\": eda_signal, \"EDA_Clean\": eda_cleaned})\n",
    "\n",
    "    signals = pd.concat([signals, eda_decomposed, peak_signal], axis=1)\n",
    "\n",
    "    return signals, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c2dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsp_process(\n",
    "    rsp_signal,\n",
    "    sampling_rate=1000,\n",
    "    method=\"khodadad2018\",\n",
    "    method_rvt=\"harrison2021\",\n",
    "    report=None,\n",
    "    **kwargs\n",
    "):\n",
    "    # Sanitize input\n",
    "    rsp_signal = nk.misc.as_vector(rsp_signal)\n",
    "    methods = nk.rsp.rsp_methods(\n",
    "        sampling_rate=sampling_rate, method=method, method_rvt=method_rvt, **kwargs\n",
    "    )\n",
    "\n",
    "    # Clean signal\n",
    "    rsp_cleaned = rsp_signal\n",
    "\n",
    "    # Extract, fix and format peaks\n",
    "    peak_signal, info = nk.rsp_peaks(\n",
    "        rsp_cleaned,\n",
    "        sampling_rate=sampling_rate,\n",
    "        method=methods[\"method_peaks\"],\n",
    "        amplitude_min=0.3,\n",
    "        **methods[\"kwargs_peaks\"],\n",
    "    )\n",
    "    info[\"sampling_rate\"] = sampling_rate  # Add sampling rate in dict info\n",
    "\n",
    "    # Get additional parameters\n",
    "    phase = nk.rsp_phase(peak_signal, desired_length=len(rsp_signal))\n",
    "    amplitude = nk.rsp_amplitude(rsp_cleaned, peak_signal)\n",
    "    rate = nk.signal.signal_rate(\n",
    "        info[\"RSP_Troughs\"], sampling_rate=sampling_rate, desired_length=len(rsp_signal)\n",
    "    )\n",
    "    symmetry = nk.rsp_symmetry(rsp_cleaned, peak_signal)\n",
    "    rvt = nk.rsp_rvt(\n",
    "        rsp_cleaned,\n",
    "        method=methods[\"method_rvt\"],\n",
    "        sampling_rate=sampling_rate,\n",
    "        silent=True,\n",
    "    )\n",
    "\n",
    "    # Prepare output\n",
    "    signals = pd.DataFrame(\n",
    "        {\n",
    "            \"RSP_Raw\": rsp_signal,\n",
    "            \"RSP_Clean\": rsp_cleaned,\n",
    "            \"RSP_Amplitude\": amplitude,\n",
    "            \"RSP_Rate\": rate,\n",
    "            \"RSP_RVT\": rvt,\n",
    "        }\n",
    "    )\n",
    "    signals = pd.concat([signals, phase, symmetry, peak_signal], axis=1)\n",
    "\n",
    "    return signals, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40b3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppg_process(\n",
    "    ppg_signal, sampling_rate=1000, method=\"elgendi\", method_quality=\"templatematch\", report=None, **kwargs\n",
    "):\n",
    "    # Sanitize input\n",
    "    ppg_signal = nk.misc.as_vector(ppg_signal)\n",
    "    methods = nk.ppg.ppg_methods(sampling_rate=sampling_rate, method=method, method_quality=method_quality, **kwargs)\n",
    "\n",
    "    # Clean signal\n",
    "    ppg_cleaned = ppg_signal\n",
    "\n",
    "    # Find peaks\n",
    "    peaks_signal, info = nk.ppg_peaks(\n",
    "        ppg_cleaned,\n",
    "        sampling_rate=sampling_rate,\n",
    "        method=\"bishop\",\n",
    "    )\n",
    "\n",
    "    info[\"sampling_rate\"] = sampling_rate  # Add sampling rate in dict info\n",
    "\n",
    "    # Rate computation\n",
    "    rate = nk.signal.signal_rate(\n",
    "        info[\"PPG_Peaks\"], sampling_rate=sampling_rate, desired_length=len(ppg_cleaned)\n",
    "    )\n",
    "\n",
    "    # Assess signal quality\n",
    "    quality = nk.ppg_quality(\n",
    "        ppg_cleaned,\n",
    "        peaks=info[\"PPG_Peaks\"],\n",
    "        sampling_rate=sampling_rate,\n",
    "        method=methods[\"method_quality\"],\n",
    "        **methods[\"kwargs_quality\"]\n",
    "    )\n",
    "\n",
    "    # Prepare output\n",
    "    signals = pd.DataFrame(\n",
    "        {\n",
    "            \"PPG_Raw\": ppg_signal,\n",
    "            \"PPG_Clean\": ppg_cleaned,\n",
    "            \"PPG_Rate\": rate,\n",
    "            \"PPG_Quality\": quality,\n",
    "            \"PPG_Peaks\": peaks_signal[\"PPG_Peaks\"].values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return signals, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d432a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str, subject_id: str, sampling_rate: int = 4):\n",
    "    data_path = Path(path) / subject_id / f'{subject_id}.pkl'\n",
    "    with open(data_path, 'rb') as file:\n",
    "        data = pickle.load(file, encoding='latin1')\n",
    "    \n",
    "    data_chest = data['signal']['chest']\n",
    "    data_wrist = data['signal']['wrist']\n",
    "    data_label = data['label']\n",
    "    \n",
    "    for key in data_chest.keys():\n",
    "        if key == 'ACC':\n",
    "            # ACC has 3 channels, so we need to resample each channel separately\n",
    "            temp = []\n",
    "            for i in range(data_chest[key].shape[1]):\n",
    "                resampled = nk.signal_resample(data_chest[key][:, i], sampling_rate=SAMPLING_RATES['chest'][key], desired_sampling_rate=SAMPLING_RATE)\n",
    "                filtered = nk.signal_filter(resampled, sampling_rate=SAMPLING_RATE, lowcut=0.4, method='fir')\n",
    "                temp.append(filtered)\n",
    "            temp.append(np.linalg.norm(np.array(temp), axis=0))\n",
    "            data_chest[key] = np.stack(temp, axis=1)\n",
    "        else:\n",
    "            resampled = nk.signal_resample(data_chest[key][:, 0], sampling_rate=SAMPLING_RATES['chest'][key], desired_sampling_rate=SAMPLING_RATE)\n",
    "            data_chest[key] = resampled\n",
    "    for key in data_wrist.keys():\n",
    "        if key == 'ACC':\n",
    "            # ACC has 3 channels, so we need to resample each channel separately\n",
    "            temp = []\n",
    "            for i in range(data_wrist[key].shape[1]):\n",
    "                temp.append(nk.signal_resample(data_wrist[key][:, i], sampling_rate=SAMPLING_RATES['wrist'][key], desired_sampling_rate=SAMPLING_RATE))\n",
    "            temp.append(np.linalg.norm(np.array(temp), axis=0))\n",
    "            data_wrist[key] = np.stack(temp, axis=1)\n",
    "        else:\n",
    "            resampled = nk.signal_resample(data_wrist[key][:, 0], sampling_rate=SAMPLING_RATES['wrist'][key], desired_sampling_rate=SAMPLING_RATE)\n",
    "            data_wrist[key] = resampled\n",
    "\n",
    "    if SAMPLING_RATES['label'] > sampling_rate:\n",
    "        data_label = data_label[::SAMPLING_RATES['label'] // sampling_rate]\n",
    "    elif SAMPLING_RATES['label'] < sampling_rate:\n",
    "        time = np.round(np.arange(0, len(data_label)) / SAMPLING_RATES['label'], 3)\n",
    "        f = scipy.interpolate.interp1d(time, data_label, kind='nearest', fill_value='extrapolate')\n",
    "        time_new = np.arange(0, time[-1], 1 / sampling_rate)\n",
    "        data_label = f(time_new)\n",
    "\n",
    "    ecg_chest, _ = ecg_process(data_chest['ECG'], sampling_rate=SAMPLING_RATE)\n",
    "    emg_chest, _ = emg_process(data_chest['EMG'], sampling_rate=SAMPLING_RATE)\n",
    "    eda_chest, _ = eda_process(data_chest['EDA'], sampling_rate=SAMPLING_RATE)\n",
    "    resp_chest, _ = rsp_process(data_chest['Resp'], sampling_rate=SAMPLING_RATE)\n",
    "    \n",
    "    bvp_wrist, _ = ppg_process(data_wrist['BVP'], sampling_rate=SAMPLING_RATE)\n",
    "    eda_wrist, _ = eda_process(data_wrist['EDA'], sampling_rate=SAMPLING_RATE)\n",
    "    \n",
    "    acc_chest = pd.DataFrame(data_chest['ACC'], columns=['ACC_x_chest', 'ACC_y_chest', 'ACC_z_chest', 'ACC_net_chest'])\n",
    "    ecg_chest = ecg_chest.drop(columns=['ECG_Raw']).add_suffix('_chest')\n",
    "    emg_chest = emg_chest.drop(columns=['EMG_Raw']).add_suffix('_chest')\n",
    "    eda_chest = eda_chest.drop(columns=['EDA_Raw']).add_suffix('_chest')\n",
    "    resp_chest = resp_chest.drop(columns=['RSP_Raw']).add_suffix('_chest')\n",
    "    temp_chest = pd.DataFrame(data_chest['Temp'], columns=['Temp_chest'])\n",
    "\n",
    "    acc_wrist = pd.DataFrame(data_wrist['ACC'], columns=['ACC_x_wrist', 'ACC_y_wrist', 'ACC_z_wrist', 'ACC_net_wrist'])\n",
    "    bvp_wrist = bvp_wrist.drop(columns=['PPG_Raw']).add_suffix('_wrist')\n",
    "    eda_wrist = eda_wrist.drop(columns=['EDA_Raw']).add_suffix('_wrist')\n",
    "    temp_wrist = pd.DataFrame(data_wrist['TEMP'], columns=['TEMP_wrist'])\n",
    "\n",
    "    label = pd.DataFrame(data_label, columns=['label'])\n",
    "    \n",
    "    # Merge all dataframes on their time indices\n",
    "    df = pd.concat([acc_chest, ecg_chest, emg_chest, eda_chest, resp_chest, temp_chest,\n",
    "                    acc_wrist, bvp_wrist, eda_wrist, temp_wrist, label], axis=1, join='outer')\n",
    "    \n",
    "    # Filter labels to include only 1 (baseline), 2 (stress), and 3 (amusement)\n",
    "    df = df[df['label'].isin([1, 2, 3])]\n",
    "    \n",
    "    # Reset index to have a clean integer index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = df.assign(\n",
    "        subject_id=subject_id,\n",
    "        label = df['label'].map({1: 0, 2: 1, 3: 0}) # Binary classification: 0 (non-stress), 1 (stress)\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05a68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(path: str, subject_ids: list, sampling_rate: int = 64):\n",
    "    all_data = {}\n",
    "    for subject_id in tqdm(subject_ids):\n",
    "        df_subject = load_data(path, subject_id, sampling_rate)\n",
    "        all_data[subject_id] = df_subject\n",
    "    \n",
    "    logger.info('All data loaded')\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c8ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(data, mean=None, std=None):\n",
    "    if mean is None:\n",
    "        mean = data.mean()\n",
    "    if std is None:\n",
    "        std = data.std()\n",
    "    data_normalized = (data - mean) / std\n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955898fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(all_data, test_subjects: int):\n",
    "    all_data_list = list(all_data.values())\n",
    "    n_subjects = len(all_data)\n",
    "    n_test = test_subjects\n",
    "    \n",
    "    n_train = n_subjects - n_test\n",
    "\n",
    "    df_train = pd.concat(all_data_list[:n_train], ignore_index=True, axis=0)\n",
    "    df_test = pd.concat(all_data_list[n_train:], ignore_index=True, axis=0)\n",
    "\n",
    "    df_train_with_anomaly = df_train.reset_index(drop=True)\n",
    "    df_train = df_train[df_train['label'] == 0].reset_index(drop=True) # Use only non-stress data for training \n",
    "    df_test = df_test.reset_index(drop=True) # Use all data for testing\n",
    "    \n",
    "    logger.info('Data split into train and test sets')\n",
    "    logger.info('Train data shape: %s', df_train.shape)\n",
    "    logger.info('Test data shape: %s', df_test.shape)\n",
    "\n",
    "    return df_train, df_train_with_anomaly, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33cb75a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [09:29<07:25, 63.58s/it]/home/peunsu/ml_project/venv/lib/python3.10/site-packages/neurokit2/events/events_find.py:152: NeuroKitWarning: No events found. Check your event_channel or adjust 'threshold' or 'keep' arguments.\n",
      "  warn(\n",
      " 67%|██████▋   | 10/15 [11:16<04:50, 58.07s/it]/home/peunsu/ml_project/venv/lib/python3.10/site-packages/neurokit2/events/events_find.py:152: NeuroKitWarning: No events found. Check your event_channel or adjust 'threshold' or 'keep' arguments.\n",
      "  warn(\n",
      " 87%|████████▋ | 13/15 [13:58<01:50, 55.07s/it]/home/peunsu/ml_project/venv/lib/python3.10/site-packages/neurokit2/events/events_find.py:152: NeuroKitWarning: No events found. Check your event_channel or adjust 'threshold' or 'keep' arguments.\n",
      "  warn(\n",
      "100%|██████████| 15/15 [15:59<00:00, 63.99s/it]\n",
      "INFO:__main__:All data loaded\n",
      "INFO:__main__:Data split into train and test sets\n",
      "INFO:__main__:Train data shape: (61696, 69)\n",
      "INFO:__main__:Test data shape: (44732, 69)\n"
     ]
    }
   ],
   "source": [
    "all_data = load_all_data('WESAD', SUBJECT_IDS, SAMPLING_RATE)\n",
    "df_train, df_train_with_anomaly, df_test = data_split(all_data, TEST_SUBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64392/885587388.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_train.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_64392/885587388.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_train_with_anomaly.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_64392/885587388.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_test.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_train.ffill(inplace=True)\n",
    "df_train_with_anomaly.ffill(inplace=True)\n",
    "df_test.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b109f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('data').mkdir(parents=True, exist_ok=True)\n",
    "df_train.to_csv('data/train.csv', index=False)\n",
    "df_train_with_anomaly.to_csv('data/train_with_anomaly.csv', index=False)\n",
    "df_test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6a11487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    61696\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dc2ea98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    61696\n",
       "1    26180\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_with_anomaly.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bcf74e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    31048\n",
       "1    13684\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45664057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a7142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
